\chapter{Tests}
\label{cha:3}
\begin{itemize}
\item
\cite{sun2019optimization}
\begin{itemize}
	\item Optimization for deep learning: theory and algorithms
	\item Goals of optimization
	\item Problem formulation
	\item Gradient descent:
	\item Tips  tricks
	\item State of the art algorithms
\end{itemize}


\item
\cite{dreyfus1990}

"The Kelley-Bryson gradient
formulas for such problems have been rediscovered by neural-
network researchers and termed back propagation"

\item
\cite{Birgin2009}

Practical Augmented lagrangian method

\item
\cite{jacot2020neural}
"Indeed
the loss surface of neural networks optimization problems is highly non-convex: it has a high number
of saddle points which may slow down the convergence (4). A number of results (3; 13; 14) suggest
that for wide enough networks, there are very few "bad" local minima, i.e. local minima with much
higher cost than the global minimum"
\item
\cite{Rumelhart1986}
Original Backpropagation paper


\end{itemize}

\section{Needed}

\begin{itemize}
\item
Basic Neural network intro

\item
Optimal control reference


\item
Simultaneous approach
\end{itemize}



%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "thesis"
%%% End: 
