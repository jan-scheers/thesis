\chapter{Numerical Experiments}
\label{cha:3}
In this chapter we shall compare the augmented Lagrangian Method with several industry standard backpropagation algorithms. We considered the following algorithms: ADAM, ...

The first comparison will be run on a small example, for which we expect that all algorithms should easily converge to a good solution.


\section{Fully connected feedforward network}
For the first comparison a similar regression problem as in chapter one will be considered. A noisy sine function will be approximated using a fully connected feedforward network with 2 hidden layers of 8 neurons wide. The network will use the ReLU activation function, which is the most used function in practice.




\begin{itemize}
\item
\cite{sun2019optimization}
\begin{itemize}
	\item Optimization for deep learning: theory and algorithms
	\item Goals of optimization
	\item Problem formulation
	\item Gradient descent:
	\item Tips  tricks
	\item State of the art algorithms
\end{itemize}


\item
\cite{dreyfus1990}

"The Kelley-Bryson gradient
formulas for such problems have been rediscovered by neural-
network researchers and termed back propagation"

\item
\cite{Birgin2009}

Practical Augmented lagrangian method

\item
\cite{jacot2020neural}
"Indeed
the loss surface of neural networks optimization problems is highly non-convex: it has a high number
of saddle points which may slow down the convergence (4). A number of results (3; 13; 14) suggest
that for wide enough networks, there are very few "bad" local minima, i.e. local minima with much
higher cost than the global minimum"
\item
\cite{Rumelhart1986}
Original Backpropagation paper


\end{itemize}

\section{Needed}

\begin{itemize}
\item
Basic Neural network intro

\item
Optimal control 

\item
Simultaneous approach
\end{itemize}



%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "thesis"
%%% End: 
