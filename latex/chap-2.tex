\chapter{Augmented Lagrangian Method}
\label{cha:2}
In this chapter the direct multiple shooting approach is examined more closely and a more specific algorithm is designed to replace \texttt{fmincon}, which is a very general method. For this problem the Augmented Lagrangian Method has been chosen. This is a common method for solving constrained Non Linear Programs (NLPs). Instead of using the classical method, an Augmented Lagrangian framework is adapted from a recent paper\cite{sahin2019}.It will be implemented in python using \texttt{numpy} v1.20.2, \texttt{scipy} v1.6.3  and \texttt{tensorflow} v2.5.0 \cite{numpy},\cite{scipy},\cite{tensorflow}.

\section{Classical Augmented Lagrangian Method}
The Augmented Lagrangian Method (ALM) is a classical algorithmic framework for solving constrained NLPs. It was first discovered in 1969 \cite{Hestenes1969},\cite{Powell1969} and was known as the method of multipliers. Textbook examples of this method can be found in \cite{Birgin2009} and \cite{bertsekas2014constrained}.

It is designed to minimize equality constrained optimization problems defined in the following way:

\begin{equation}
	\begin{aligned}
	& \underset{u}{\text{min}} & f(u) & \\
	& \text{s.t.} & h(u) &= 0 \\
	\end{aligned}
\end{equation}

ALM solves this by minimizing a series of unconstrained problems in a similar manner as the penalty method. In each iteration a $\beta$-augmented Lagrangian $\mathcal{L}_\beta(x,\lambda)$ is minimized for x:

\begin{equation}
	\underset{u}{\text{min}} \hspace{.5em} \underset{\lambda}{\text{max}} \hspace{.5em}  \mathcal{L}_\beta(u,\lambda) = f(u) + \langle\lambda,h(u)\rangle + \frac{\beta}{2} || h(u) ||^2_2
\end{equation}

where $\beta>0$ is the penalty weight. This can be viewed as a penalty method which has been shifted using the term in $\lambda$\cite{Birgin2009}. When $\beta$ or $\lambda$ tend to infinity, $h(u)$ will be forced to zero, leading the Lagrangian to converge to the same solution as the original problem.

The algorithm proceeds as follows:
\begin{gather*}
	u_{k+1} = \underset{u}{\text{argmin}} \hspace{.5em} \mathcal{L}_\beta(u,\lambda_k) \\
	\lambda_{k+1} = \lambda_k + \sigma_k h(u_{k+1})
\end{gather*}

where $\sigma_k$ is the step size at iteration $k$. Then in each step the penalty parameter $\beta_k$ is increased or kept the same, depending on the size of the constraint violation. This continues until an acceptable solution has been found:

\begin{equation}
	||h(u_k)|| \leq \tau_1 \hspace{.5em}\text{and}\hspace{.5em} ||\nabla_u\mathcal{L}_{\beta_k}(u_k,\lambda_k)|| \leq \tau_2
\end{equation}

with $\tau_1,\tau_2$ the chosen tolerances.

\section{Applied Augmented Lagrangian Method}

The OCP equation \ref{ocp-eq} of training a neural net with MSE loss function is a constrained nonlinear least squares(LS) problem:
\begin{equation*}
	\begin{aligned}
	& \underset{W}{\text{minimize}}
	& & \sum\limits_{j=0}^{n}||\sigma_L(W_Lz_L) - y_j||^2_2 \\
	& \text{subject to}
	& & z_{1,j} = \sigma_0(W_0,x_j), &j = 1,\ldots,n \\
	& & & z_{k+1,j} = \sigma_k(W_kz_{k,j}), &k = 1,\ldots,L-1,j = 1,\ldots,n \\
    & & \Updownarrow \\
	& \text{min}
	&  & \frac{1}{2} ||F(u)||^2_2 \\
	& \text{s. t.}
	& &  h(u) = 0
	\end{aligned}
\end{equation*}
Where $u = \{W,z\}$ is the collection of both the weight and state variables into a single vector.

The subproblem that will be solved in each iteration is then:
\begin{equation}
	\begin{aligned}
	 & \underset{u}{\text{argmin}} & \mathcal{L}_{\beta}(u,\lambda) 
	     &= \frac{1}{2} ||F(u)||^2_2 + \langle\lambda,h(u)\rangle + \frac{\beta}{2} || h(u) ||^2_2 \\
	 & & &= \frac{1}{2} ||F(u)||^2_2 + \frac{\beta}{2} ||h(u) + \lambda/\beta ||^2_2 - \frac{1}{2\beta} ||\lambda||^2_2 \\
	 & & &= \frac{\beta}{2} \Big|\Big|
		\begin{bmatrix}
			F(u)/\sqrt{\beta} \\
			h(u) + \lambda/\beta
		\end{bmatrix} \Big|\Big|^2_2 \\
	\end{aligned}
	\label{loss}
\end{equation}

Instead of using the textbook algorithm, an algorithmic framework from a more recent paper\cite{sahin2019} is adapted to the problem, shown in Algorithm \ref{algo}.

\begin{algorithm}[H]
\SetAlgoLined
\SetKw{Kw}{Initialization}
\SetKwComment{Comment}{}{}
\SetCommentSty{emph}
\KwIn{Initial weights vector $W$, penalty parameter $\beta$, stopping tolerance $\tau$, input-target pairs $(x_i,y_i), i = 1,\ldots,n$}
\Kw{$u_0 = \{W,f_{W}(x)\}, \lambda_0 \in \mathcal{N}(0,1)$}
\Comment*[r]{Initialize state variables by simulating network, initialize dual variables randomly}
\For{k = 0,1,...}{
 	$\eta_k = 1/\beta^k$
 	\Comment*[r]{Update tolerance}
 	find $u_{k+1}$ such that \\
 	\Indp$||\nabla_{u_k}\mathcal{L}_{\beta^k}(u_k,\lambda_k)|| \leq \eta_k$ \label{ls-prob}
 	\Comment*[r]{Approx. primal solution}
 	\Indm$\sigma_{k+1} = \text{min}\big(\frac{||h(u_0)||\log^22}{||h(u_{k+1})||k \log^2(k+1)},1\big)$
 	\Comment*[r]{Update dual step size}
 	$\lambda_{k+1} = \lambda_k + \sigma_{k+1}h(u_{k+1})$
 	\Comment*[r]{Update dual variables}
 	$||\nabla_{u_{k+1}}\mathcal{L}_{\beta^k}(u_{k+1},\lambda_k)|| + ||h(u_{k+1})||<\tau$
 	\Comment*[r]{Stopping Criterion}
 	
 }
 \caption{Inexact Augmented Lagrangian Method}
 \label{algo}
\end{algorithm}
The penalty parameter increases geometrically, $\beta_k = \beta_0^k$, and the tolerence decreases geometrically $\eta_k = 1/\beta_k$. It is called the inexact Augmented Lagrangian Method (iALM) because the optimizer $u^*$ of subproblem \ref{loss} can only be solved to an approximate solution.  The choice of dual step size $\sigma_k$ is to ensure the boundedness of the dual variables $\lambda_k$ \cite{sahin2019},\cite{bertsekas1976}. 


Figure \ref{nabla} shows the convergence behaviour of this algorithm. In this figure the algorithm was run for 10 epochs on a neural network training problem. The gradient of the $\beta$-Augmented Lagrangian is plotted in blue.

\begin{equation}
||\nabla_{u_k}\mathcal{L}_{\beta^k}(u_k,\lambda_k)|| = ||2(\nabla_{u_k}\begin{bmatrix} F(u)/\sqrt{\beta} \\ h(u) + \lambda/\beta \end{bmatrix})\mathcal{L}_{\beta^k}(u_k,\lambda_k)||
\end{equation}

The gradient decreases geometrically as the tolerence is decreased in each step $\eta_{k+1} = \eta_k/\beta$. The MSE loss of the network is plotted in red. It is calculated by taking the current optimal weights at that epoch and simulating the network on the training data. In this example the MSE loss reaches a minimum after 6 iterations, which is a typical result.

The constraint violations, and the variables associated with the states are not relevant when evaluating the performance of the network. For this reason the stopping criterion in Algorithm \ref{algo} may not be the most practical choice. In deep learning many different stopping criteria are used. Often these are based on the training loss, or the loss on a validation set which is held apart from the training data. Usually a tradeoff will have to be made between training performance and overfitting the data (Goodfellow et al. \cite{Goodfellow-et-al-2016}, Sec. 8.1). This is a practical issue, in the next chapter the problem of choosing an appropriate stopping criterion is examined more fully.

\begin{figure}
	\centering
	\includegraphics[width=.7\textwidth]{nabla}
	\caption{Typical convergence behaviour of Algorithm \ref{algo}}
	\label{nabla}
\end{figure}

\section{Least Squares Solver}
To solve the LS problem \ref{loss} in Algorithm \ref{algo}, a Trust Region Reflective(\texttt{trf}) method is used, which is implemented in \texttt{scipy.optimize.least\_squares} \cite{scipyls}. The following description is given by \texttt{scipy}: "The algorithm iteratively solves trust-region subproblems augmented by a special diagonal quadratic term and with trust-region shape determined by the distance from the bounds and the direction of the gradient." The \texttt{trf} method is described as being robust for both bounded and unbounded problems, and well suited for sparse Jacobians. 

The LS problem \ref{loss} is an unbounded problem for which the library recommends using a Levenberg-Marquardt method. However this method cannot handle cases where the Jacobian has more columns than rows, which can sometimes occur depending on the size of the network and the number of data points used for training. Therefore we cannot use the Levenberg-Marquardt algorithm.

To efficiently solve the least squares problem, the solver requires an analytical solution for the Jacobian, which will be explained in the next section.

\section{Jacobian}

To solve the least squares problem, the Jacobian matrix of $M_{\beta}(u,\lambda) = \begin{bmatrix} F(u)/\sqrt{\beta} \\ h(u) + \lambda/\beta \end{bmatrix}$ must be calculated. A Jacobian is the matrix of all partial derivatives of a vector valued function. In this case:

\begin{equation}
J_{M_{\beta}} = 
\begin{bmatrix}
\frac{\partial{M_{\beta}}}{\partial u_1} & 
\frac{\partial{M_{\beta}}}{\partial u_2} & ... & 
\frac{\partial{M_{\beta}}}{\partial u_n} \\
\end{bmatrix}
\end{equation}

 It has a relatively sparse structure because there are no distant connections in the neural net, each layer is only connected to the next one and the previous one. In this section the partial derivative associated with each variable will be presented.
 
 First the columns of $J_{M_{\beta}}$ associated with the weight variables will be examined:
 
\begin{equation}
	\frac{\partial{M_{\beta}}}{\partial W_k} = -z_{k,j}\sigma'_k(W_kz_{k,j}), k = 0,\ldots,L, j = 1,\ldots,n
\end{equation}

$W_k$ is a matrix of size $d_{k+1}\times(d_k+1)$, and $z_{k,j}$ are vectors of size $d_k+1$, therefore this evaluates as a 3D tensor. $W_k$ must be vectorized first to allow this derivative to be used in the Jacobian. After vectorization the dimensions of this partial derivative as a block matrix are: $d_{k+1}n \times d_{k+1}(d_{k}+1)$. The last partial derivative $\frac{\partial{M_{\beta}}}{\partial W_L}$ is also multiplied by a factor $\frac{1}{\beta}$.

 Next the columns of $J_{M_{\beta}}$ associated with the states $z_k$ are examined:
 
 \begin{equation}
 	\frac{\partial{M_{\beta}}}{\partial z_k} = \begin{bmatrix} 1 \\ -W_k\sigma'_k(W_kz_k) \end{bmatrix}, k = 1,\ldots,L
 \end{equation}
 $W_k$ are as before and $z_k$ are matrices of size $(d_k+1)\times n$. The row of ones associated with the biases vector in $W_k$ is not a variable so it is excluded from the jacobian. After vectorization of $z_k$ the partial derivative has dimensions $(d_k+d_{k+1})n\times d_kn$.
 
For a fully connected neural network with identity output activation an example has been written out in Table \ref{jac-tab}. Figure \ref{jac} shows a visual representation of the matrix, where the nonzero elements have been colored black. An alternative representation, which is mathematically the same is to swap the rows corresponding to the loss function $F(u)$ to the bottom. This gives a matrix with a banded structure, which is is plotted in figure \ref{jac2}. This is the representation used in the code.

\begin{figure}[p]
	\centering
	\begin{subfigure}{\textwidth}
	  \centering
	  \includegraphics[width=\textwidth]{jac0.png}
	  \caption{Jacobian matrix}
	  \label{jac}
	\end{subfigure}
	\begin{subfigure}{\textwidth}
	  \centering
	  \includegraphics[width=\textwidth]{jac1.png}
	  \caption{Jacobian matrix, rearranged}
	  \label{jac2}
	\end{subfigure}
	\caption{Visual representation of Jacobian matrix for network with 2 inputs, 2 outputs, width 3, depth 2 and 7 datapoints. The non-zero elements have been colored black. The Jacobian at the top is the same as the one on the bottom, but with swapped rows.}
	\label{jactot}
\end{figure}

Consider a feedforward network with input dimension I, an output dimension O, and D hidden layers of width W. The weight matrixes have $I \cdot W + O \cdot W + (D-1) \cdot W \cdot W$ parameters, the bias vectors have $D \cdot W+O$ parameters and the state vectors have $D \cdot W \cdot N$ parameters. On the other hand $M_{\beta}(u,\lambda)$ has an output dimension of $D \cdot W \cdot N + O \cdot N$. The dimension of the Jacobian for this network is therefore :
\begin{equation}
(D \cdot W \cdot N + O \cdot N) \times (D \cdot W \cdot N + O + (D+I+O) \cdot W + (D-1) \cdot W^2)
\label{jdim}
\end{equation}. 

The Jacobian scales quadratically in size with the depth of the network and the number of datapoints. It scales cubically with the width of the network. The Jacobian will have more rows than columns when:
\begin{equation}
	N \geq 1 + (D+I+O) \cdot W/O + (D-1) \cdot W^2/O
\end{equation}



\section{Numerical verification of Jacobian Matrix}
In the previous section the Jacobian matrix was derived analytically. In this section will be explained how the Jacobian is verified algorithmically.

Algorithmic Differentation (AD) is a set of techniques which can be used to calculate the derivative of any computer code \cite{Rall1981}, \cite{wikiad}. Because all code is composed of elementary operations, AD can use the chain rule alongside the operations to automatically compute derivatives of arbitrary order. By injecting code from an AD library into the calculation of the neural network, the Jacobian can be calculated numerically. For this the AlgoPy python library was used \cite{algopy}. The output of the AD was then compared to the analytical result for a number of different network configurations, confirming them to be equal within a small tolerance. The code is shown in Appendix \ref{AD}.

\begin{table}[p]
\tiny
\centering

\begin{subtable}{\textwidth}
\makebox[\textwidth][c]{
\begin{tabular}{r r | c c c c c}

\multicolumn{7}{c}{Weight variables, each entry is a block matrix} \\ \hline

& $\nabla^T_{W_0,b_0}M$ & $W_{0_1}$ & $W_{0_2}$ & ... & $W_{0_W}$ & $b_0$ \\
& dim & I & I &...& I & W \\ \hline
$F$ & O*N & 0 & 0 &...& 0 & 0\\ \hline
$h_1$ & N & 		$-x\sigma'(W_{0_1}x+b_{0_1})$ & 0 &...& 0 & $-\sigma'(W_{0_1}x+b_{0_1})$ \\
      & N & 0 & 	$-x\sigma'(W_{0_2}x+b_{0_2})$ &...& 0 &  	$-\sigma'(W_{0_2}x+b_{0_2})$ \\
      &...&...&...&...&...&... \\
      & N & 0 & 0 &...& $-x\sigma'(W_{0_W}x+b_{0_W})$ &  		$-\sigma'(W_{0_W}x+b_{0_W})$ \\ \hline
$h_2$ & W*N & 0 & 0 &...& 0 & 0 \\
...   & ... &...&...&...&...&...\\ 
$h_{D}$ & W*N & 0 & 0 &...& 0 & 0 \\ \hline \\ \hline

& $\nabla^T_{W_i,b_i}M$ & $W_{i_1}$ & $W_{i_2}$ &...& $W_{i_W}$ & $b_i$ \\
& dim & W & W &...& W & W \\ \hline
$F$ & O*N & 0 & 0 &...& 0 & 0 \\ \hline
$h_1$ & W*N & 0 & 0 &...& 0 & 0 \\
...   & ... &...&...&...&...&...\\ \hline
$h_{i+1}$ & N & 		$-z_1\sigma'(W_{i_1}z + b_{i_1})$ & 0 &...& 0 & $-\sigma'(W_{i_1}x+b_{i_1})$ \\
      & N & 0 & 	$-z_1\sigma'(W_{i_2}z + b_{i_2})$ &...& 0 & 	$-\sigma'(W_{i_2}x+b_{i_2})$ \\
      &...&...&...&...&...&... \\
      & N & 0 & 0 &...& $-z_1\sigma'(W_{i_W}z + b_{i_W})$ & 		$-\sigma'(W_{i_W}x+b_{i_W})$ \\ \hline
...   & ... &...&...&...&...&...\\ 
$h_{D}$ & W*N & 0 & 0 &...& 0 & 0 \\ \hline \\ \hline

& $\nabla^T_{W_D,b_D}M$ & $W_{D_1}$ &  $W_{D_2}$  &...&  $W_{D_O}$ & $b_D$ \\
& dim & W & W &...& W & O \\ \hline
$F$ & N & $-\frac{z_D}{\sqrt{c}}\sigma_O'(W_{D_1}x+b_{D_1})$ & 0 &...& 0 & $-\frac{1}{\sqrt{c}}\sigma_O'(W_{D_1}x+b_{D_1})$ \\
    & N & 0 & $-\frac{z_D}{\sqrt{c}}\sigma_O'(W_{D_2}x+b_{D_2})$ &...& 0 & $-\frac{1}{\sqrt{c}}\sigma_O'(W_{D_2}x+b_{D_2})$ \\
      &...&...&...&...&...&... \\
    & N & 0 & 0 &...& $-\frac{z_D}{\sqrt{c}}\sigma_O'(W_{D_O}x+b_{D_O})$ & $-\frac{1}{\sqrt{c}}\sigma_O'(W_{D_O}x+b_{D_O})$ \\ \hline
$h_1$ & W*N & 0 & 0 &...& 0 & 0 \\
...   & ... &...&...&...&...&...\\ 
$h_{D}$ & W*N & 0 & 0 &...& 0 & 0 \\ \hline
      
\end{tabular}}
\end{subtable}


\begin{subtable}{\textwidth}
\makebox[\textwidth][c]{
\begin{tabular}{ r r | c c c c }
\multicolumn{6}{c}{State variables, each entry is a diagonal matrix} \\ \hline
& $\nabla^T_{z_i}M$ & $z_{i_1}$ & $z_{i_2}$ &...& $z_{i_W}$\\
&  dim & N & N &...& N \\ \hline
$F$ & O*N & 0 & 0 &...& 0 \\ \hline
$h_1$ & W*N & 0 & 0 &...& 0 \\
...   & ... &...&...&...&...\\\hline
$h_i$ & N & 1 & 0 &...& 0 \\
      & N & 0 & 1 &...& 0  \\
      &...&...&...&...&...\\ 
      & N & 0 & 0 &...& 1  \\ \hline
$h_{i+1}$ & N & $-W_{i_{1,1}}\sigma'(W_{i_1}z_i+b_{i_1})$ & $-W_{i_{1,2}}\sigma'(W_{i_1}z_i+b_{i_1})$ &...& $-W_{i_{1,W}}\sigma'(W_{i_1}z_i+b_{i_1})$\\
          & N & $-W_{i_{2,1}}\sigma'(W_{i_2}z_i+b_{i_2})$ & $-W_{i_{2,2}}\sigma'(W_{i_2}z_i+b_{i_2})$ &...& $-W_{i_{2,W}}\sigma'(W_{i_2}z_i+b_{i_2})$\\
      &...&...&...&...&...\\ 
          & N & $-W_{i_{W,1}}\sigma'(W_{i_W}z_i+b_{i_W})$ & $-W_{i_{W,2}}\sigma'(W_{i_W}z_i+b_{i_W})$ &...& $-W_{i_{W,W}}\sigma'(W_{i_W}z_i+b_{i_W})$\\ \hline
...   & ... &...&...&...&...\\ 
$h_{D}$ & W*N & 0 & 0 &...& 0 \\ \hline \\
& $\nabla^T_{z_D}M$ & $z_{D_1}$ & $z_{D_2}$ &...& $z_{D_W}$\\
& dim & N & N & ... &  N \\ \hline
F & N &         $-W_{D_{1,1}}\sigma_O'(W_{D_1}z_D+b_{D_1})$ & $-W_{D_{1,2}}\sigma_O'(W_{D_1}z_D+b_{D_1})$ &...& $-W_{D_{1,W}}\sigma_O'(W_{D_1}z_D+b_{D_1})$\\
          & N & $-W_{D_{2,1}}\sigma_O'(W_{D_2}z_D+b_{D_2})$ & $-W_{D_{2,2}}\sigma_O'(W_{D_2}z_D+b_{D_2})$ &...& $-W_{D_{2,W}}\sigma_O'(W_{D_2}z_D+b_{D_2})$\\
      &...&...&...&...&...\\ 
          & N & $-W_{D_{O,1}}\sigma_O'(W_{D_O}z_D+b_{D_O})$ & $-W_{D_{O,2}}\sigma_O'(W_{D_O}z_D+b_{D_O})$ &...& $-W_{D_{O,W}}\sigma_O'(W_{D_O}z_D+b_{D_O})$\\ \hline
$h_1$ & W*N & 0 & 0 &...& 0 \\
...   & ... &...&...&...&...\\\hline
$h_D$ & N & 1 & 0 &...& 0 \\
      & N & 0 & 1 &...& 0  \\
      &...&...&...&...&...\\ 
      & N & 0 & 0 &...& 1  \\ \hline
\end{tabular}}
\end{subtable}
\caption{Jacobian of feedforward neural network. In this table the biases are not included in the weight matrices $W_k$. Each layer has the same width $W$ and same activation $\sigma$. There are $D$ layers. The input dimension is $I$ and the output dimension is $O$. There are $N$ datapoints.}
\label{jac-tab}

\end{table}
\section{Conclusion}
In this chapter an Augmented Lagrangian framework was proposed to solve the Optimal Control Problem. A Least Squares solver was applied to the LS problem at the heart of the algorithm, and a Jacobian Matrix was analytically derived which is supplied to the solver. The Jacobian was also algorithmically verified. In the next chapter the algorithm will be investigated using numerical tests and compared against an industry standard optimizer.



