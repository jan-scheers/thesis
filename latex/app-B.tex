\chapter{Source code}
\label{app:B}
This appendix contains the source code of the Augmented lagrangian method and the automatic verification of the Jacobian Matrix
\footnotesize
\section{Augmented lagrangian method}
\begin{verbatim}
#alm.py
import numpy as np
import tensorflow as tf
from tensorflow import keras
from scipy import sparse
import scipy.optimize as op
import scipy.linalg as la

class Dense_d(keras.layers.Dense):
    def __init__(self,activation_,**kwargs):
        super().__init__(**kwargs)
        self.activation_ = activation_

class ALMModel:
    def __init__(self,model,x,y):
        self.model = model
        t = self.model(x)
        self.x = x
        self.y = y
        self.batch_size = x.shape[0]
        self.nz = sum([self.batch_size*model.layers[i].units for i in range(len(model.layers)-1)])

    def read(self,u):
        layers = self.model.layers
        w = []
        p = 0
        for i in range(0,len(layers)):
            s = layers[i].weights[0].shape
            m,n = s[1],s[0]+1
            w.append(u[p:p+m*n].reshape(m,n))
            p = p+m*n

        z = [self.x.transpose()]
        for i in range(1,len(layers)):
            m,n = w[i].shape[1]-1,self.batch_size
            zi = u[p:p+m*n].reshape(m,n)
            z.append(zi)
            p = p+m*n
        y = np.r_[self.y.transpose()]
        z.append(y)

        return w,z
    
    def write(self):
        u = np.empty(0)
        for layer in self.model.layers:
            w,b = layer.weights
            m,n = w.shape[1],w.shape[0]+1
            w = np.c_[w.numpy().transpose(),b.numpy().reshape(m,1)]
            u = np.append(u,w)
        z = [self.x]
        for i,layer in enumerate(self.model.layers):
            z.append(layer(z[i]))

        z = z[1:-1]
        z = [zi.numpy().transpose() for zi in z] 
        u = np.append(u,np.concatenate(z))
        return u

    def set_weights(self,w):
        for i,layer in enumerate(self.model.layers):
            wi = w[i].transpose()
            layer.set_weights([wi[:-1,:],wi[-1:,].reshape((-1,))])

    def h(self,u):
        w,z = self.read(u)
        h = []
        for i,layer in enumerate(self.model.layers[:-1]):
            zi_e = np.r_[z[i],np.ones((1,self.batch_size))]
            hi = z[i+1] - np.array(layer.activation(w[i].dot(zi_e)))
            h.append(hi.reshape(-1))
        return np.concatenate(h)

    def L(self,u,beta,l):
        w,z = self.read(u)
        
        out_layer = self.model.layers[-1]
        z_e = np.r_[z[-2],np.ones((1,self.batch_size))]
        F = z[-1] - np.array(out_layer.activation(w[-1].dot(z_e)))
        F = F.reshape(-1)/np.sqrt(beta)

        h = self.h(u)+l/beta
        return np.concatenate([h,F])

        
    def JL(self,u,beta):
        layers = self.model.layers
        w,z = self.read(u)

        w_ = []
        z_ = []
        np.set_printoptions(linewidth=200)
        for i in range(len(layers)):
            # Add bias to weights
            zi_e = np.r_[z[i],np.ones((1,self.batch_size))]

            ai = layers[i].activation_(w[i].dot(zi_e))

            wi_ = -zi_e*ai[:,np.newaxis,:]
            wi_ = wi_*np.eye(w[i].shape[0])[:,:,np.newaxis,np.newaxis]
            wi_ = np.swapaxes(wi_,1,2).reshape(w[i].size,w[i].shape[0]*self.batch_size).transpose()

            w_.append(wi_)

            if i > 0:
                zi_ = -w[i][:,:-1].transpose()[:,:,np.newaxis]*ai
                zi_ = np.eye(self.batch_size)*zi_[:,:,:,np.newaxis]
                zi_ = np.swapaxes(zi_,1,2).reshape(z[i].size,z[i+1].size).transpose()
                z_.append(zi_)

        
        w_[-1] = w_[-1]/np.sqrt(beta)
        z_[-1] = z_[-1]/np.sqrt(beta)

        w_ = [sparse.csc_matrix(wi_) for wi_ in w_]
        z_ = [sparse.csc_matrix(zi_) for zi_ in z_]
        
        z_ = sparse.block_diag(z_)
        z_ = sparse.vstack((sparse.csr_matrix((w_[0].shape[0],z_.shape[1])),z_))
        z_ = z_+sparse.eye(z_.shape[0],z_.shape[1])
        w_ = sparse.block_diag(w_)
        J = sparse.hstack((w_,z_))
        return J
            

    def fit_alm(self,val_data=None,beta=10,tau=1e-2):
        l = np.random.normal(0,1,self.nz)
        u = self.write()
        sigma_0,h_0 = 1,la.norm(self.h(u))
        hist = {"tol":np.empty(0),"njev":np.empty(0),"loss":np.empty(0),'val_loss':np.empty(0)}
        for k in range(10):
            beta_k = np.power(beta,k)
            eta_k = 1/beta_k
            fun = lambda u: self.L(u,beta_k,l)
            jac = lambda u: self.JL(u,beta_k)
            try:
                sol = op.least_squares(fun,u,jac,ftol=None,xtol=None,gtol=eta_k,tr_solver='lsmr')
            except:
                print("Divide by 0")
                break
            u = sol.x

            w,_ = self.read(u)
            self.set_weights(w)

            h = self.h(u)

            sigma = sigma_0*np.amin([h_0*np.power(np.log(2),2)/la.norm(h)/(k+1)/np.power(np.log(k+2),2),
            																						 1])
            l = l + sigma*h

            # Convergence of Lagrangian function
            jac = self.JL(u,beta_k).toarray()
            tol = la.norm(2*self.JL(u,beta_k).transpose()*self.L(u,beta_k,l))+la.norm(h)
            hist['tol'] = np.append(hist['tol'],tol)
            hist['njev'] = np.append(hist['njev'],sol.njev)

            # Convergence of training loss
            y_pred = self.model(self.x)
            mse = keras.losses.MeanSquaredError()
            loss = mse(self.y,y_pred).numpy()
            hist['loss'] = np.append(hist['loss'],loss)
            if(val_data):
                # Convergence of val/test loss
                y_val_pred = self.model(val_data[0])
                val_loss = mse(y_val_pred,val_data[1]).numpy()
                print("epoch: ",k+1,"DL: ",tol,"njev: ",sol.njev, 
                	  "loss = ",loss,"val_loss = ",val_loss)
                hist['val_loss'] = np.append(hist['val_loss'],val_loss)

            if k>1 and ((1+tau)*hist['loss'][-1] > hist['loss'][-2]):
                break
        return hist

\end{verbatim}

\subsection{Numerical verification of Jacobian Matrix}
\label{AD}
\begin{verbatim}
# almtest.py
import net
from algopy import UTPM
import tensorflow as tf
import numpy as np
from tensorflow import keras
import alm
from matplotlib import pyplot

I,O,W,D,N, = 2,1,3,2,21
mu = 10

shape = (W,D,I,O)
sigma  = lambda x: np.tanh(x)
sigma_ = lambda x: 2/(np.cosh(2*x)+1)

tau  = lambda x: x
tau_ = lambda x: np.ones(x.shape)

x = np.random.normal(0,1,I*N).reshape((I,N))
y = np.random.normal(0,1,O*N).reshape((O,N))
u = np.random.random_sample((W*W*(D-1) + D*W + I*W + O*W + O + D*W*N,))
l = np.random.random_sample((D*W*N,))

nn = net.Net(shape,sigma,sigma_,tau,tau_,x,y)

_,z = nn.sim(u)
n_u = u.size
u[n_u-W*D*N:n_u] = z.ravel()

L = nn.eval_L(u,mu,l)
J = nn.eval_J_L(u,mu,l)

u_U = UTPM.init_jacobian(u)
L_U = net.eval_L_U(u_U,mu,l,nn)
J_U = UTPM.extract_jacobian(L_U)
J_U = J_U.reshape(J_U.shape[1:3])


model = keras.Sequential()
model.add(alm.Dense_d(activation_=sigma_,units=W,activation=tf.math.tanh,input_shape=(I,)))
model.add(alm.Dense_d(activation_=sigma_,units=W,activation=tf.math.tanh))
model.add(alm.Dense_d(activation_=tau_,units=O,activation=tau))
n2 = alm.ALMModel(model, x.transpose(), y.transpose())
model.summary()

g = np.arange(W)+1
a = g*(I+1)
b = a[2] + g*(W+1)
c = b[2] + W+1

g = np.r_[a,b,c]-1
h = np.arange(n_u-D*W*N,n_u)

mask = np.ones((n_u,),bool)

mask[g] = 0
mask[h] = 0

k = np.arange(n_u)
t = np.arange(n_u)
t[mask] = k[0:h[0]-D*W-O]
t[g] = k[h[0]-D*W-O:h[0]]
t[h] = k[h]

L2 = n2.L(u[t], mu, l)
J2 = n2.JL(u[t], mu).toarray()

Jnz = J2 != 0

J2 = np.c_[J2[:,mask],J2[:,g],J2[:,h]]

print(np.allclose(L,np.ravel(L_U.data[0,0])))
print(np.allclose(L,L2))
print(np.allclose(J,J_U))
print(np.allclose(J,J2))

np.set_printoptions(precision=3,linewidth=200)
pyplot.matshow(Jnz,cmap="binary")
pyplot.show()

\end{verbatim}

\section{batch tests}
\subsection{tanh test}
\begin{verbatim}
import net
from algopy import UTPM
import tensorflow as tf
import numpy as np
from tensorflow import keras
import alm
from matplotlib import pyplot
import time

rng = np.random.default_rng()
delta = 0.1
W = 8
K = 20

sigma  = lambda x: tf.math.tanh(x)
sigma_ = lambda x: 2/(np.cosh(2*x)+1)

tau  = lambda x: x
tau_ = lambda x: np.ones(x.shape)


for N in [10,20,40]:
    x = np.linspace(0,np.pi,N).reshape((N,1))
    y = np.sin(x*x)+rng.normal(0,delta,x.shape)
    per = rng.permutation(N)

    te = np.empty((K,4))
    for k in range(K):
        madam = keras.Sequential() 
        madam.add(keras.layers.Dense(activation="tanh",units=W,input_shape=(x.shape[1],)))
        madam.add(keras.layers.Dense(activation="tanh",units=W))
        madam.add(keras.layers.Dense(units=y.shape[1]))
        adam = keras.optimizers.Adam()
        es = keras.callbacks.EarlyStopping(monitor='loss',patience=10)
        madam.compile(optimizer=adam, loss='mean_squared_error')

        malm = keras.Sequential()
        malm.add(alm.Dense_d(activation=sigma,activation_=sigma_,units=W,input_shape=(x.shape[1],)))
        malm.add(alm.Dense_d(activation=sigma,activation_=sigma_,units=W))
        malm.add(alm.Dense_d(activation=tau,activation_=tau_,units=y.shape[1]))
    
        w = madam.get_weights()
        malm.set_weights(w)
        
        t0 = time.process_time()
        hist = madam.fit(x[per,:],y[per,:],batch_size=N,epochs=4000,callbacks=[es],verbose=0)
        t1 = time.process_time()-t0
        e1 = len(hist.history['loss'])
        print("t: ",t1,"k: ",e1,'loss: ', hist.history['loss'][-1])

        almnet = alm.ALMModel(malm, x[per,:], y[per,:])

        t0 = time.process_time()
        hist2 = almnet.fit_alm()
        t2 = time.process_time()-t0
        e2 = hist2['loss'].size
        print("t: ",t2,"k: ",e2, 'loss: ', hist2['loss'][-1])
    
        te[k,:] = [t1,t2,e1,e2]
        with open('testtanhn.npy','ab') as f:
            np.save(f,np.array(hist.history['loss']))
            np.save(f,np.concatenate([np.array(value) for value in 
                                      hist2.values()]).reshape((3,-1)).transpose())
    with open('testtanhn.npy','ab') as f:
        np.save(f,te)

\end{verbatim}
\subsection{relu test}
\begin{verbatim}
import net
from algopy import UTPM
import tensorflow as tf
import numpy as np
from tensorflow import keras
import alm
from matplotlib import pyplot
import time

rng = np.random.default_rng()
delta = 0.1
W = 16
K = 20

sigma  = lambda x: tf.nn.relu(x)
sigma_ = lambda x: np.greater(x,0,x)

tau  = lambda x: x
tau_ = lambda x: np.ones(x.shape)


for N in [20,40,80]:
    x = np.linspace(0,np.pi,N).reshape((N,1))
    y = np.sin(x*x)+rng.normal(0,delta,x.shape)
    per = rng.permutation(N)

    te = np.empty((K,4))
    for k in range(K):
        madam = keras.Sequential() 
        madam.add(keras.layers.Dense(activation="relu",units=W,input_shape=(x.shape[1],)))
        madam.add(keras.layers.Dense(activation="relu",units=W))
        madam.add(keras.layers.Dense(units=y.shape[1]))
        adam = keras.optimizers.Adam()
        es = keras.callbacks.EarlyStopping(monitor='loss',patience=10)
        madam.compile(optimizer=adam, loss='mean_squared_error')

        malm = keras.Sequential()
        malm.add(alm.Dense_d(activation=sigma,activation_=sigma_,units=W,input_shape=(x.shape[1],)))
        malm.add(alm.Dense_d(activation=sigma,activation_=sigma_,units=W))
        malm.add(alm.Dense_d(activation=tau,activation_=tau_,units=y.shape[1]))
    
        w = madam.get_weights()
        malm.set_weights(w)
        
        t0 = time.process_time()
        hist = madam.fit(x[per,:],y[per,:],batch_size=N,epochs=4000,callbacks=[es],verbose=0)
        t1 = time.process_time()-t0
        e1 = len(hist.history['loss'])
        print("t: ",t1,"k: ",e1,'loss: ', hist.history['loss'][-1])

        almnet = alm.ALMModel(malm, x[per,:], y[per,:])

        t0 = time.process_time()
        hist2 = almnet.fit_alm()
        t2 = time.process_time()-t0
        e2 = hist2['loss'].size
        print("t: ",t2,"k: ",e2, 'loss: ', hist2['loss'][-1])
    
        te[k,:] = [t1,t2,e1,e2]
        with open('testrelun.npy','ab') as f:
            np.save(f,np.array(hist.history['loss']))
            np.save(f,np.concatenate([np.array(value) for value in 
                                      hist2.values()]).reshape((3,-1)).transpose())
    with open('testrelun.npy','ab') as f:
        np.save(f,te)
\end{verbatim}


