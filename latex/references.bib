@Book{h2g2,
  author =    {Adams, Douglas},
  title =     {The Hitchhiker's Guide to the Galaxy},
  publisher = {Del Rey (reprint)},
  year =      1995,
  note =      {ISBN-13: 978-0345391803}}

@Book{pratchett06:_good_omens,
  author =    {Pratchett, Terry and Gaiman, Neil},
  title =     {Good Omens:
               \emph{The Nice and Accurate Prophecies of Agnes Nutter, Witch}},
  publisher = {HarperTorch (reprint)},
  year =      2006,
  note =      {ISBN-13: 978-0060853983}}

@Misc{wiki,
  author =       {Wikipedia},
  title =        {Thesis or dissertation},
  howpublished = {URL: \url{http://en.wikipedia.org/wiki/Thesis_or_dissertation},
                  last checked on 2010-01-07}}
 
@misc{sun2019optimization,
      title={Optimization for deep learning: theory and algorithms}, 
      author={Ruoyu Sun},
      year={2019},
      eprint={1912.08957},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


@article{dreyfus1990,
author = {Dreyfus, Stuart E.},
title = {Artificial neural networks, back propagation, and the Kelley-Bryson gradient procedure},
journal = {Journal of Guidance, Control, and Dynamics},
volume = {13},
number = {5},
pages = {926-928},
year = {1990},
doi = {10.2514/3.25422},

URL = {
        https://doi.org/10.2514/3.25422

},
eprint = {
        https://doi.org/10.2514/3.25422

}

}

@Inbook{Birgin2009,
author="Birgin, Ernesto G.
and Mart{\'i}nez, J. M.",
editor="Floudas, Christodoulos A.
and Pardalos, Panos M.",
title="Practical augmented Lagrangian methodsPractical Augmented Lagrangian Methods",
bookTitle="Encyclopedia of Optimization",
year="2009",
publisher="Springer US",
address="Boston, MA",
pages="3013--3023",
abstract="Keywords",
isbn="978-0-387-74759-0",
doi="10.1007/978-0-387-74759-0_517",
url="https://doi.org/10.1007/978-0-387-74759-0_517"
}

@misc{jacot2020neural,
      title={Neural Tangent Kernel: Convergence and Generalization in Neural Networks},
      author={Arthur Jacot and Franck Gabriel and Cl√©ment Hongler},
      year={2020},
      eprint={1806.07572},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@Article{Rumelhart1986,
author={Rumelhart, David E.
and Hinton, Geoffrey E.
and Williams, Ronald J.},
title={Learning representations by back-propagating errors},
journal={Nature},
year={1986},
month={Oct},
day={01},
volume={323},
number={6088},
pages={533-536},
abstract={We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal `hidden' units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure1.},
issn={1476-4687},
doi={10.1038/323533a0},
url={https://doi.org/10.1038/323533a0}
}

@INPROCEEDINGS{mizutani2000,

  author={E. {Mizutani} and S. E. {Dreyfus} and K. {Nishio}},

  booktitle={Proceedings of the IEEE-INNS-ENNS International Joint Conference on Neural Networks. IJCNN 2000. Neural Computing: New Challenges and Perspectives for the New Millennium}, 

  title={On derivation of MLP backpropagation from the Kelley-Bryson optimal-control gradient formula and its application}, 

  year={2000},

  volume={2},

  number={},

  pages={167-172 vol.2},

  doi={10.1109/IJCNN.2000.857892}}


@article{bock1984multiple,
  title={A multiple shooting algorithm for direct solution of optimal control problems},
  author={Bock, Hans Georg and Plitt, Karl-Josef},
  journal={IFAC Proceedings Volumes},
  volume={17},
  number={2},
  pages={1603--1608},
  year={1984},
  publisher={Elsevier}
}



@book{Goodfellow-et-al-2016,
    title={Deep Learning},
    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher={MIT Press},
    note={\url{http://www.deeplearningbook.org}},
    year={2016}
}

@book{Nielsen2015,
    title={Neural Networks and Deep Learning},
    author={Michael A. Nielsen},
    publisher={Determination Press},
    note={\url{http://neuralnetworksanddeeplearning.com}},
    year={2015}
}

@misc{kingma2017adam,
      title={Adam: A Method for Stochastic Optimization},
      author={Diederik P. Kingma and Jimmy Ba},
      year={2017},
      eprint={1412.6980},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@Article{Powell1969,
author="POWELL, M. J. D.",
title="A method for nonlinear constraints in minimization problems",
journal="Optimization",
ISSN="",
publisher="Academic Press",
year="1969",
month="",
volume="",
number="",
pages="283-298",
DOI="",
}

@Article{Hestenes1969,
author={Hestenes, Magnus R.},
title={Multiplier and gradient methods},
journal={Journal of Optimization Theory and Applications},
year={1969},
month={Nov},
day={01},
volume={4},
number={5},
pages={303-320},
abstract={The main purpose of this paper is to suggest a method for finding the minimum of a functionf(x) subject to the constraintg(x)=0. The method consists of replacingf byF=f+$\lambda$g+1/2cg2, wherec is a suitably large constant, and computing the appropriate value of the Lagrange multiplier. Only the simplest algorithm is presented. The remaining part of the paper is devoted to a survey of known methods for finding unconstrained minima, with special emphasis on the various gradient techniques that are available. This includes Newton's method and the method of conjugate gradients.},
issn={1573-2878},
doi={10.1007/BF00927673},
url={https://doi.org/10.1007/BF00927673}
}

@book{bertsekas2014constrained,
  title={Constrained optimization and Lagrange multiplier methods},
  author={Bertsekas, Dimitri P},
  year={2014},
  publisher={Academic press}
}

