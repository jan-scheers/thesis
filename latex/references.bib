@Book{h2g2,
  author =    {Adams, Douglas},
  title =     {The Hitchhiker's Guide to the Galaxy},
  publisher = {Del Rey (reprint)},
  year =      1995,
  note =      {ISBN-13: 978-0345391803}}

@Book{pratchett06:_good_omens,
  author =    {Pratchett, Terry and Gaiman, Neil},
  title =     {Good Omens:
               \emph{The Nice and Accurate Prophecies of Agnes Nutter, Witch}},
  publisher = {HarperTorch (reprint)},
  year =      2006,
  note =      {ISBN-13: 978-0060853983}}

 
@Misc{wiki,
  author =       {Wikipedia},
  title =        {Thesis or dissertation},
  howpublished = {URL: \url{http://en.wikipedia.org/wiki/Thesis_or_dissertation},
                  last checked on 2010-01-07}}
@misc{sun2019optimization,
      title={Optimization for deep learning: theory and algorithms}, 
      author={Ruoyu Sun},
      year={2019},
      eprint={1912.08957},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}


@article{dreyfus1990,
author = {Dreyfus, Stuart E.},
title = {Artificial neural networks, back propagation, and the Kelley-Bryson gradient procedure},
journal = {Journal of Guidance, Control, and Dynamics},
volume = {13},
number = {5},
pages = {926-928},
year = {1990},
doi = {10.2514/3.25422},

URL = {
        https://doi.org/10.2514/3.25422

},
eprint = {
        https://doi.org/10.2514/3.25422

}

@book{Rall1981,
series = {Lecture notes in computer science 120},
publisher = {Springer},
isbn = {3540108610},
year = {1981},
title = {Automatic differentiation : techniques and applications},
address = {Berlin},
author = {Rall, Louis B},
keywords = {Ordinary differential equations: boundary value problems; convergence and stability; error analysis; initial value problems; multistep methods; single step methods; stiff equations (Numerical analysis)},
}

}

@Inbook{Birgin2009,
author="Birgin, Ernesto G.
and Mart{\'i}nez, J. M.",
editor="Floudas, Christodoulos A.
and Pardalos, Panos M.",
title="Practical augmented Lagrangian methodsPractical Augmented Lagrangian Methods",
bookTitle="Encyclopedia of Optimization",
year="2009",
publisher="Springer US",
address="Boston, MA",
pages="3013--3023",
abstract="Keywords",
isbn="978-0-387-74759-0",
doi="10.1007/978-0-387-74759-0_517",
url="https://doi.org/10.1007/978-0-387-74759-0_517"
}

@misc{jacot2020neural,
      title={Neural Tangent Kernel: Convergence and Generalization in Neural Networks},
      author={Arthur Jacot and Franck Gabriel and Clément Hongler},
      year={2020},
      eprint={1806.07572},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@Article{Rumelhart1986,
author={Rumelhart, David E.
and Hinton, Geoffrey E.
and Williams, Ronald J.},
title={Learning representations by back-propagating errors},
journal={Nature},
year={1986},
month={Oct},
day={01},
volume={323},
number={6088},
pages={533-536},
abstract={We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal `hidden' units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure1.},
issn={1476-4687},
doi={10.1038/323533a0},
url={https://doi.org/10.1038/323533a0}
}

@INPROCEEDINGS{mizutani2000,

  author={E. {Mizutani} and S. E. {Dreyfus} and K. {Nishio}},

  booktitle={Proceedings of the IEEE-INNS-ENNS International Joint Conference on Neural Networks. IJCNN 2000. Neural Computing: New Challenges and Perspectives for the New Millennium}, 

  title={On derivation of MLP backpropagation from the Kelley-Bryson optimal-control gradient formula and its application}, 

  year={2000},

  volume={2},

  number={},

  pages={167-172 vol.2},

  doi={10.1109/IJCNN.2000.857892}}


@article{bock1984multiple,
  title={A multiple shooting algorithm for direct solution of optimal control problems},
  author={Bock, Hans Georg and Plitt, Karl-Josef},
  journal={IFAC Proceedings Volumes},
  volume={17},
  number={2},
  pages={1603--1608},
  year={1984},
  publisher={Elsevier}
}



@book{Goodfellow-et-al-2016,
    title={Deep Learning},
    author={Ian Goodfellow and Yoshua Bengio and Aaron Courville},
    publisher={MIT Press},
    note={\url{http://www.deeplearningbook.org}},
    year={2016}
}

@book{Nielsen2015,
    title={Neural Networks and Deep Learning},
    author={Michael A. Nielsen},
    publisher={Determination Press},
    note={\url{http://neuralnetworksanddeeplearning.com}},
    year={2015}
}

@misc{kingma2017adam,
      title={Adam: A Method for Stochastic Optimization},
      author={Diederik P. Kingma and Jimmy Ba},
      year={2017},
      eprint={1412.6980},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@Article{Powell1969,
author="POWELL, M. J. D.",
title="A method for nonlinear constraints in minimization problems",
journal="Optimization",
ISSN="",
publisher="Academic Press",
year="1969",
month="",
volume="",
number="",
pages="283-298",
DOI="",
}

@Article{Hestenes1969,
author={Hestenes, Magnus R.},
title={Multiplier and gradient methods},
journal={Journal of Optimization Theory and Applications},
year={1969},
month={Nov},
day={01},
volume={4},
number={5},
pages={303-320},
abstract={The main purpose of this paper is to suggest a method for finding the minimum of a functionf(x) subject to the constraintg(x)=0. The method consists of replacingf byF=f+$\lambda$g+1/2cg2, wherec is a suitably large constant, and computing the appropriate value of the Lagrange multiplier. Only the simplest algorithm is presented. The remaining part of the paper is devoted to a survey of known methods for finding unconstrained minima, with special emphasis on the various gradient techniques that are available. This includes Newton's method and the method of conjugate gradients.},
issn={1573-2878},
doi={10.1007/BF00927673},
url={https://doi.org/10.1007/BF00927673}
}

@book{bertsekas2014constrained,
  title={Constrained optimization and Lagrange multiplier methods},
  author={Bertsekas, Dimitri P},
  year={2014},
  publisher={Academic press}
}

@inproceedings{sahin2019,
 author = {Sahin, Mehmet Fatih and eftekhari, Armin and Alacaoglu, Ahmet and Latorre, Fabian and Cevher, Volkan},
 booktitle = {Advances in Neural Information Processing Systems},
 editor = {H. Wallach and H. Larochelle and A. Beygelzimer and F. d\textquotesingle Alch\'{e}-Buc and E. Fox and R. Garnett},
 pages = {},
 publisher = {Curran Associates, Inc.},
 title = {An  Inexact Augmented Lagrangian Framework for Nonconvex Optimization with Nonlinear Constraints},
 url = {https://proceedings.neurips.cc/paper/2019/file/866c7ee013c58f01fa153a8d32c9ed57-Paper.pdf},
 volume = {32},
 year = {2019}
}

@article{bertsekas1976,
issn = {0363-0129},
abstract = {In this paper we consider a generalized class of quadratic penalty function methods for the solution of nonconvex nonlinear programming problems. This class contains as special cases both the usual quadratic penalty function method and the recently proposed multiplier method. We obtain convergence and rate of convergence results for the sequences of primal and dual variables generated. The convergence results for the multiplier method are global in nature and constitute a substantial improvement over existing local convergence results. The rate of convergence results show that the multiplier method should be expected to converge considerably faster than the pure penalty method. At the same time, we construct a global duality framework for nonconvex optimization problems. The dual functional is concave, everywhere finite, and has strong differentiability properties. Furthermore, its value, gradient and Hessian matrix within an arbitrary bounded set can be obtained by unconstrained minimization of a certain augmented Lagrangian.},
journal = {SIAM journal on control and optimization},
pages = {216--235},
volume = {14},
publisher = {Society for Industrial and Applied Mathematics},
number = {2},
year = {1976},
title = {On Penalty and Multiplier Methods for Constrained Minimization},
copyright = {[Copyright] © 1976 Society for Industrial and Applied Mathematics},
language = {eng},
address = {Philadelphia},
author = {Bertsekas, Dimitri P},
}

@misc{dertat2017, 
  title={Applied Deep Learning - Part 1: Artificial Neural Networks}, 
  howpublished={URL: \url{https://towardsdatascience.com/applied-deep-learning-part-1-artificial-neural-networks-d7834f67a4f6}, retrieved 2021-05-31}, 
  publisher={Towards Data Science}, 
  author={Dertat, Arden}, 
  year={2017}, 
  month={Oct}
}

@misc{scipyls,
    title={scipy.optimize.least\_squares},
    howpublished={URL: \url{https://docs.scipy.org/doc/scipy/reference/generated/scipy.optimize.least_squares.html}, retrieved 2021-05-31}
    author={scipy community}
}

@article{Lu2020,
   title={Dying ReLU and Initialization: Theory and Numerical Examples},
   volume={28},
   ISSN={1991-7120},
   url={http://dx.doi.org/10.4208/cicp.OA-2020-0165},
   DOI={10.4208/cicp.oa-2020-0165},
   number={5},
   journal={Communications in Computational Physics},
   publisher={Global Science Press},
   author={Lu, Lu},
   year={2020},
   month={Jun},
   pages={1671-1706}
}


@Misc{dertat2017,
  author =       {kipedia},
  title =        {Thesis or dissertation},
  howpublished = {URL: \url{http://en.wikipedia.org/wiki/Thesis_or_dissertation},
                  last checked on 2010-01-07}}
